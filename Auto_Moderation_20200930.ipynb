{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4a4566b9554fc6890f1931f5242cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1601581867580_0001</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import packages\n",
    "#import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import  Normalizer, LemmatizerModel, StopWordsCleaner, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, VectorAssembler, HashingTF, IDF, Tokenizer\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1406905c54744598be01f85e7d43fc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Snowflake connection info\n",
    "sfOptions = {\n",
    "  'sfURL' : 'powerreviews.snowflakecomputing.com',\n",
    "  'sfAccount' : 'powerreviews',\n",
    "  'sfUser' : 'APP_SPARK_PROD',\n",
    "  'sfPassword' : '>6txhcQ*7&^Qrq`j',\n",
    "  'sfDatabase' : 'ANALYTICS_PROD',\n",
    "  'sfSchema' : 'ANALYTICS',\n",
    "  'sfWarehouse' : 'LOWER_ENV_WAREHOUSE'\n",
    "}\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09588118829402499dddbd21cb65cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_old = \"select * \\\n",
    "from \\\n",
    "(select ugc_id as mod_ugc_id, created_date as mod_date, year(created_date)*100+month(created_date) as YYYYMM \\\n",
    "from cdm.observation as o \\\n",
    "where observation_type_code = 'PM' and observation_client_id =1 and ugc_type = 'review') as moderated_reviews \\\n",
    "left join \\\n",
    "(select ugc_id, observation_type_code \\\n",
    "from cdm.observation as o \\\n",
    "where observation_type_code in ('NR','PII','PR','CS','DC','CR','FRD','TST','CC','EWL','SC','PIM','LC','WEB','FL','SA','URL','CV','US') \\\n",
    " and observation_client_id =1 and ugc_type = 'review') as observation_reviews \\\n",
    "on moderated_reviews.mod_ugc_id = observation_reviews.ugc_id \\\n",
    "left join \\\n",
    "(select ugc_id as r_ugc_id, review_headline, review_comments, review_rating, reviewer_nickname, locale \\\n",
    "from analytics.review ) as r \\\n",
    "on r.r_ugc_id = moderated_reviews.mod_ugc_id \\\n",
    "where locale in ('en_US','en_GB','en_CA') \\\n",
    "and mod_date > '2020-09-21'   \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d20e443233483a9d14924d81de0bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"select distinct r.ugc_id, r.created_date, r.review_headline, \\\n",
    "r.review_comments, r.review_rating, r.reviewer_nickname, r.locale, r.observations, \\\n",
    "case when contains(r.observations, 'PM') then 1 else 0 end as PM, \\\n",
    "case when contains(r.observations, 'NR') then 1 else 0 end as NR, \\\n",
    "case when contains(r.observations, 'PII') then 1 else 0 end as PII, \\\n",
    "case when contains(r.observations, 'PR') then 1 else 0 end as PR, \\\n",
    "case when contains(r.observations, 'CS') then 1 else 0 end as CS, \\\n",
    "case when contains(r.observations, 'DC') then 1 else 0 end as DC, \\\n",
    "case when contains(r.observations, 'CR') then 1 else 0 end as CR, \\\n",
    "case when contains(r.observations, 'FRD') then 1 else 0 end as FRD, \\\n",
    "case when contains(r.observations, 'TST') then 1 else 0 end as TST, \\\n",
    "case when contains(r.observations, 'CC') then 1 else 0 end as CC, \\\n",
    "case when contains(r.observations, 'SC') then 1 else 0 end as SC, \\\n",
    "case when contains(r.observations, 'PIM') then 1 else 0 end as PIM, \\\n",
    "case when contains(r.observations, 'LC') then 1 else 0 end as LC, \\\n",
    "case when contains(r.observations, 'WEB') then 1 else 0 end as WEB, \\\n",
    "case when contains(r.observations, 'FL') then 1 else 0 end as FL, \\\n",
    "case when contains(r.observations, 'SA') then 1 else 0 end as SA, \\\n",
    "case when contains(r.observations, 'URL') then 1 else 0 end as URL, \\\n",
    "case when contains(r.observations, 'CV') then 1 else 0 end as CV, \\\n",
    "case when contains(r.observations, 'US') then 1 else 0 end as US, \\\n",
    "case when r.observations = 'PM' then 1 else 0 end as NOLABEL \\\n",
    "from analytics.review as r \\\n",
    "left join cdm.observation as o \\\n",
    "on r.ugc_id = o.ugc_id \\\n",
    "where contains(r.observations, 'PM') \\\n",
    "and r.locale in ('en_US','en_GB','en_CA') \\\n",
    "and o.created_date > '2020-09-23'  \\\n",
    "limit 100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a85ea3d2f804d10a8f81752e4d4c87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_testing = \"select distinct r.ugc_id, r.created_date, r.review_headline, \\\n",
    "r.review_comments, r.review_rating, r.reviewer_nickname, r.reviewer_location, r.observations, \\\n",
    "case when contains(r.observations, 'PM') then 1 else 0 end as PM, \\\n",
    "case when contains(r.observations, 'NR') then 1 else 0 end as NR, \\\n",
    "case when contains(r.observations, 'PII') then 1 else 0 end as PII, \\\n",
    "case when contains(r.observations, 'PR') then 1 else 0 end as PR, \\\n",
    "case when contains(r.observations, 'CS') then 1 else 0 end as CS, \\\n",
    "case when contains(r.observations, 'DC') then 1 else 0 end as DC, \\\n",
    "case when contains(r.observations, 'CR') then 1 else 0 end as CR, \\\n",
    "case when contains(r.observations, 'FRD') then 1 else 0 end as FRD, \\\n",
    "case when contains(r.observations, 'TST') then 1 else 0 end as TST, \\\n",
    "case when contains(r.observations, 'CC') then 1 else 0 end as CC, \\\n",
    "case when contains(r.observations, 'SC') then 1 else 0 end as SC, \\\n",
    "case when contains(r.observations, 'PIM') then 1 else 0 end as PIM, \\\n",
    "case when contains(r.observations, 'LC') then 1 else 0 end as LC, \\\n",
    "case when contains(r.observations, 'WEB') then 1 else 0 end as WEB, \\\n",
    "case when contains(r.observations, 'FL') then 1 else 0 end as FL, \\\n",
    "case when contains(r.observations, 'SA') then 1 else 0 end as SA, \\\n",
    "case when contains(r.observations, 'URL') then 1 else 0 end as URL, \\\n",
    "case when contains(r.observations, 'CV') then 1 else 0 end as CV, \\\n",
    "case when contains(r.observations, 'US') then 1 else 0 end as US, \\\n",
    "case when r.observations = 'PM' then 1 else 0 end as NOLABEL \\\n",
    "from analytics.review as r \\\n",
    "where r.locale in ('en_US','en_GB','en_CA') \\\n",
    "and r.created_date > '2018-09-01'  \\\n",
    "and contains(r.observations, 'PM') \\\n",
    "limit 100000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68857952525d4a10a322a766032fdd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use review data set to get observation codes bc they are already comma separated and less joins to perform\n",
    "#Still need observation set though to get observation created date\n",
    "#This is bc review could be from ages ago but we could have imported recently and moderated it\n",
    "#Want to make sure we include as many observation labels as possible for most accurate model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d0731cda2b478dbbaacde497da3818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+-------------+-----------------+-----------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+\n",
      "|   UGC_ID|        CREATED_DATE|     REVIEW_HEADLINE|     REVIEW_COMMENTS|REVIEW_RATING|REVIEWER_NICKNAME|REVIEWER_LOCATION|OBSERVATIONS| PM| NR|PII| PR| CS| DC| CR|FRD|TST| CC| SC|PIM| LC|WEB| FL| SA|URL| CV| US|NOLABEL|\n",
      "+---------+--------------------+--------------------+--------------------+-------------+-----------------+-----------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+\n",
      "|234199352|2019-01-29 00:00:...|         Comfy shoes|My husband loves ...|            5|          Giselle|      San Juan PR|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|\n",
      "|234259318|2019-01-29 00:00:...|      Yes yes yes!!!|I saw that kids s...|            5|      Jenni jenni|      Chicago, IL|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|\n",
      "|234263776|2019-01-29 00:00:...|       Great sneaker|My 5 year old son...|            5|       Michelle L|       Sutton, MA|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|\n",
      "|234264753|2019-01-29 00:00:...|Fits better than ...|I order XS becaus...|            5|           SAMIRA|       California|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|\n",
      "|234279710|2019-01-29 00:00:...|          Fab shoes!|Loved the shoes a...|            5|           Red_75|           Dublin|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|\n",
      "+---------+--------------------+--------------------+--------------------+-------------+-----------------+-----------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "100000"
     ]
    }
   ],
   "source": [
    "#Spark data frame\n",
    "\n",
    "reviews = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "          .options(**sfOptions).option(\"query\", query_testing).load()\n",
    "reviews.show(5)\n",
    "reviews.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a24decf8ae4c8ca27b5876c85bf90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ALL_TEXT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|headline Not on clearance comment These are showing up in your clearance area but are sold at full price.Bait and switch.  Expecting better than that from Shuler. nickname Shoe liver location Minnesota                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|headline Really quality gear, perfect layers. comment The Icebreaker natural base layers are perfect for cold to extremely cold conditions whether you'll be putting in miles in the backcountry or just spending some quality time in the outdoors. The layers are made with a high degree of detail and you can feel the difference. These base layers are my go to and will even where them over adding mid-layers during warmer temps. Try them for yourself, you won't regret it. nickname Brewtality location Bend, OR                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|headline Just right for me comment I bought this long sleeve base layer for a winter trip to Europe. In test runs before the trip it has felt snug an warm and the zipper provides a good fit around. So far so good. nickname GryW location California                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|headline Seem like good tights for a variety of situations comment I've worn these under goretex outdooors and under chino's to work when we had heating issues, pretty handy tights and the price is not too bad compared to some of the higher end offerings from icebreaker. I'd recommend these over poly tights of similar weight (like the cheaper but inferior heattech from Uniclo). Found all icebreaker long johns to fit a bit small so i sized up to XL and was happy. nickname Workcommuter01 location Vancouver, BC, Canada                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|headline Great value and comfort! comment Well sewn and comfortable fit. Appropriate for weather conditions as outlined by manufacturer. nickname Desperado location Hamilton,Ontario                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|headline Outstanding Quality. comment Amazing Quality and Very Comfortable.\n",
      "\n",
      "However, it is stated that the hat is reversible, though to get the inner label out requires removing some stitches. nickname MrKC location United Kingdom                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|headline Really quality gear, perfect layers. comment The Icebreaker natural base layers are perfect for cold to extremely cold conditions whether you'll be putting in miles in the backcountry or just spending some quality time in the outdoors. The layers are made with a high degree of detail and you can feel the difference. These base layers are my go to and will even where them over adding mid-layers during warmer temps. Try them for yourself, you won't regret it. nickname Brewtality location Bend, OR                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|headline Skiing ready comment Just wore skiing at Crested Butte. Performed as advertised. Now I need more nickname Gagolfer location Louisville, KY                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|headline Perfect fit and so comfortable! comment These boots are so beautiful and comfortable. Couldn't  be happier with this purchase! nickname Lespoodles location Pinellas Park FL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|headline beautiful shoe but slips on the heels comment I love these shoes. They're a great, very versatile color and style. My only issue is the heel slippage. I have rather narrow heels and these tend to slip on my heel a lot despite the fact that they fit perfectly in size. I haven't had this issue with other Frye shoes before, but heel pads seem to help some. nickname photogmel location South Carolina                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|headline Great design comment Bottles were great- cute design that set us apart from other families at daycare. Compact for 9 oz- which made transporting to and from daycare and in diaper bag easy! Very easy to put together (much more streamlined than compared to other top anti reflux bottles). This made the valves so much easier to clean as well! Overall highly satisfied and no longer using my old bottles! Received from weespring parent panel. nickname Sarahp1027 location Rockville Md                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|headline Excellent comment I love it. It does a stream powerful work. It clean very good. nickname Chayotita2010 location Arlington                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|headline Great Product comment We vacuum 6-8 vehicles a day and this thing is a beast!! Highly recommended! nickname TopCop location Illinois                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|headline Go to hiking sock comment These socks are quickly becoming my go to hiking sock - the weight is just right. I have several pairs. nickname AnneCordelia location Ontario                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|headline Wear for a week sking and it stays fresh . comment Have bought a few of these over the years for skiing and walking .Great product. Fit used to be on the small side but with sleeves far too long. The one I just bought is perfect both in the body and sleeve so the design has improved.\n",
      "Would recommend it to everyone. nickname skicoop location Scotland                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|headline perfect for winter outdoor activities comment Living in Minnesota, all my layers have to count! I use mostly icebreaker for everything. This is a new addition. Fits beautifully. All I need now is snow. nickname christy location Minneapolis, MN, USA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|headline better than expected. comment i prefer to shop in the store. My store of choice is Green Hills in Nashville, TN. They always take great care in making certain i find exactly what i am looking for. This Christmas we decided to purchase a gift card for my father in law. While i was in the store the Surrey with the commando soles caught my eye. They did not have one in my size to i decided to order it in. The shipment was quick and i was notified. To my great surprise this boot in tan is softer and better than looking than i expected. I was very impressed. This is now my fourth pair of AE shoes and there will be more purchased. The quality is incredible and the comfort can not be understated. If you have not tried on a pair do yourself a favor. These round out the wardrobe and set one apart from the crowd. nickname anonymous location Nashville, TN|\n",
      "|headline Best Steam Generator Iron comment Bought one of these in Turkey in September for use in our holiday home to replace the steam generator which had broken albeit £100 cheaper in Turkey than the UK price, my wife was so impressed with the performance she wanted one for use here in England so bought one in Argos, without a doubt the best steam generator iron we have ever had. Can't understand why the vast pricing difference though. nickname Kanne48 location Bowers Gifford, Essex                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|headline Love it comment I wish i would known earlier that this lamp exist, it makes dark mornings so much easier and help to fall asleep much quicker by making sunset. love the natural sounds, really helps wake up with less stress as possible, as previously with phone alarm it seemed like my heard will jump out and run away. \n",
      "Lovely light, well done recreating sunset and sunrise. Bought one also for friends, they loved it too as they cant get out often and it can get depressed at home, so they really enjoyed this lamp and how warm it makes the room. \n",
      "Really good product, glad i got it nickname Skul location Kent                                                                                                                                                                                                                                                   |\n",
      "|headline Great little thing comment Great night light, good design, looks classy and works well! nickname Dazzui location London                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "#Change locations with just 'US' to 'USA' otherwise the lemmatizer will change 'us' to 'we'\n",
    "reviews = reviews.withColumn('LOCATION_UPDATED', F.when(F.col('REVIEWER_LOCATION')=='US', 'USA').otherwise(F.col('REVIEWER_LOCATION'))) \\\n",
    "                    .withColumn('ALL_TEXT', F.concat(F.lit('headline '),'REVIEW_HEADLINE', F.lit(' comment '), 'REVIEW_COMMENTS', \\\n",
    "                             F.lit(' nickname '),'REVIEWER_NICKNAME', F.lit(' location '), 'LOCATION_UPDATED')) \\\n",
    "                    .drop('REVIEW_HEADLINE','REVIEW_COMMENTS','REVIEWER_NICKNAME','LOCATION_UPDATED','REVIEWER_LOCATION')\n",
    "#reviews.select('REVIEW_HEADLINE','REVIEW_COMMENTS','ALL_TEXT').show(truncate=False)\n",
    "reviews.select('ALL_TEXT').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680f3dc702b44e62859b9124c8af1ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+--------------------+-----+\n",
      "|   UGC_ID|       CREATED_DATE|REVIEW_RATING|OBSERVATIONS| PM| NR|PII| PR| CS| DC| CR|FRD|TST| CC| SC|PIM| LC|WEB| FL| SA|URL| CV| US|NOLABEL|            ALL_TEXT|PRICE|\n",
      "+---------+-------------------+-------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+--------------------+-----+\n",
      "|233995151|2019-01-29 00:00:00|            5|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline I live t...|    0|\n",
      "|234033292|2019-01-29 00:00:00|            5|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline Love lov...|    0|\n",
      "|234134471|2019-01-29 00:00:00|            5|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline Absolute...|    0|\n",
      "|234138060|2019-01-29 00:00:00|            5|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline Lov this...|    0|\n",
      "|234263848|2019-01-29 00:00:00|            5|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline GREAT BU...|    0|\n",
      "|225067329|2019-01-29 05:50:22|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline One of m...|    0|\n",
      "|234252169|2019-01-29 00:00:00|            5|       CM,PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline I would ...|    0|\n",
      "|225096584|2019-01-29 15:26:02|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline WOW. Ser...|    0|\n",
      "|225099458|2019-01-29 17:12:14|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline Best Pac...|    0|\n",
      "|225092902|2019-01-29 13:41:05|            1|       NR,PM|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline I never ...|    0|\n",
      "|225079846|2019-01-29 10:28:42|            3|      PM,WEB|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|      0|headline Misled c...|    0|\n",
      "|225064290|2019-01-29 04:53:10|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline Fabulous...|    0|\n",
      "|225095482|2019-01-29 14:46:46|            1|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline They wer...|    0|\n",
      "|225065135|2019-01-29 05:14:40|            3|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline I would ...|    0|\n",
      "|225080544|2019-01-29 10:53:31|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline Good cus...|    0|\n",
      "|225090840|2019-01-29 13:02:04|            1|    CM,NR,PM|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline terrible...|    0|\n",
      "|225101944|2019-01-29 19:09:42|            5|    CM,PM,PO|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|headline Sweet Me...|    0|\n",
      "|225082290|2019-01-29 12:00:18|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline I would ...|    0|\n",
      "|225100818|2019-01-29 18:11:11|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline Great pr...|    0|\n",
      "|225087130|2019-01-29 12:34:27|            5|          PM|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      1|headline Excellen...|    0|\n",
      "+---------+-------------------+-------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+--------------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "#Look for text that contains specific price mention (9.99 = yes, 20 = no)\n",
    "reviews_2 = reviews.withColumn('PRICE', F.when(F.col('ALL_TEXT').rlike('\\d+(?:[.]\\d{2})')==False,0).otherwise(1))\n",
    "reviews_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1baa963db7f34384acb7885974c2cbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spark-nlp package can only process after text has been turned into a \"document\"\n",
    "documentAssembler = DocumentAssembler().setInputCol('ALL_TEXT') .setOutputCol('DOCUMENT_RAW')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077d722c194a4a12812056a893447fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#If this gives setInputCols error - run import of Tokenizer again\n",
    "from sparknlp.annotator import  Tokenizer\n",
    "tokenizer = Tokenizer().setInputCols(['DOCUMENT_RAW']).setOutputCol('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057fe943667b4b7db378a7625735c47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(stopwords.words('english'))\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_cleaner = StopWordsCleaner().setInputCols(['TOKEN']).setOutputCol('TOKEN_NOSTOPWORDS').setStopWords(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a20ed0c47847c690fcc27759407704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Normalizing removes punctuation like commas and periods, good to have\n",
    "#But also removes @ from @ss, or * from f*ck leaving @ss as 'ss' and f*ck as 'fck'\n",
    "#http://www.amazon.com becomes httpwwwamazoncom\n",
    "#But as long as this is consistent then model should still learn to associate 'ss' and 'fck' as profanity\n",
    "\n",
    "#Normalizing completely removes $9.99 and price references, removes emojis\n",
    "#NOT good for price label or for profanity label\n",
    "\n",
    "#setCleanupPatterns(patterns): Regular expressions list for normalization, defaults [^A-Za-z]\n",
    "#.setCleanupPatterns([\"[^\\w\\d\\s]\"]).  removes not word, not  digit, not white space\n",
    "#try removing only ,.!-'  but keep $*:/        regex: [\\,\\.\\!\\-\\']\n",
    "#This will remove normal sentence ending punct and commas but KEEPS @ss and f*ck and http://wwwamazoncom and $999 and emojis\n",
    "normalizer = Normalizer().setInputCols(['TOKEN_NOSTOPWORDS']).setOutputCol('NORMALIZED').setLowercase(True) \\\n",
    "  .setCleanupPatterns([\"[\\,\\.\\!\\-\\'\\(\\)\\?]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b9dac039fa427fa4b56957c09ee513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]"
     ]
    }
   ],
   "source": [
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(['NORMALIZED']).setOutputCol('LEMMA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df01f332e994d54aa514aad81918a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Finisher - puts transformed columns into human readable form\n",
    "finisher = Finisher().setInputCols(['TOKEN','TOKEN_NOSTOPWORDS','NORMALIZED','LEMMA']).setCleanAnnotations(False)\n",
    "pipeline = Pipeline().setStages([documentAssembler, tokenizer, stopwords_cleaner, normalizer, lemmatizer, finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227470ef99f34411a52c06e92f4e386e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+--------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n",
      "|UGC_ID   |CREATED_DATE       |REVIEW_RATING|OBSERVATIONS|PM |NR |PII|PR |CS |DC |CR |FRD|TST|CC |SC |PIM|LC |WEB|FL |SA |URL|CV |US |NOLABEL|ALL_TEXT                                                                              |PRICE|finished_TOKEN                                                                                         |finished_TOKEN_NOSTOPWORDS                                                                 |finished_NORMALIZED                                                                  |finished_LEMMA                                                                     |\n",
      "+---------+-------------------+-------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+--------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n",
      "|232210703|2019-02-27 11:18:40|5            |IR,PM       |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0      |headline Love it! comment Thanks, it is great nickname Determined location undisclosed|0    |[headline, Love, it, !, comment, Thanks, ,, it, is, great, nickname, Determined, location, undisclosed]|[headline, Love, !, comment, Thanks, ,, great, nickname, Determined, location, undisclosed]|[headline, love, comment, thanks, great, nickname, determined, location, undisclosed]|[headline, love, comment, thank, great, nickname, determine, location, undisclosed]|\n",
      "+---------+-------------------+-------------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+--------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "#Run reviews through cleaning pipeline\n",
    "reviews_3 = pipeline.fit(reviews_2).transform(reviews_2)\n",
    "reviews_4 = reviews_3.drop('DOCUMENT_RAW','TOKEN','TOKEN_NOSTOPWORDS','NORMALIZED','LEMMA')\n",
    "reviews_4.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8e90cd1b3c4fc68e1fd64eb4284e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 6 more fields]\n",
      "+- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 11 more fields]\n",
      "   +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 10 more fields]\n",
      "      +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 9 more fields]\n",
      "         +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 8 more fields]\n",
      "            +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 7 more fields]\n",
      "               +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 6 more fields]\n",
      "                  +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 5 more fields]\n",
      "                     +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 4 more fields]\n",
      "                        +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 3 more fields]\n",
      "                           +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 2 more fields]\n",
      "                              +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ALL_TEXT#1204]\n",
      "                                 +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_HEADLINE#1004, REVIEW_COMMENTS#1005, REVIEW_RATING#1006, REVIEWER_NICKNAME#1007, REVIEWER_LOCATION#1008, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, ... 6 more fields]\n",
      "                                    +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_HEADLINE#1004, REVIEW_COMMENTS#1005, REVIEW_RATING#1006, REVIEWER_NICKNAME#1007, REVIEWER_LOCATION#1008, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, ... 5 more fields]\n",
      "                                       +- Relation[UGC_ID#1002,CREATED_DATE#1003,REVIEW_HEADLINE#1004,REVIEW_COMMENTS#1005,REVIEW_RATING#1006,REVIEWER_NICKNAME#1007,REVIEWER_LOCATION#1008,OBSERVATIONS#1009,PM#1010,NR#1011,PII#1012,PR#1013,CS#1014,DC#1015,CR#1016,FRD#1017,TST#1018,CC#1019,SC#1020,PIM#1021,LC#1022,WEB#1023,FL#1024,SA#1025,... 4 more fields] SnowflakeRelation\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "UGC_ID: decimal(38,0), CREATED_DATE: timestamp, REVIEW_RATING: decimal(38,0), OBSERVATIONS: string, PM: decimal(1,0), NR: decimal(1,0), PII: decimal(1,0), PR: decimal(1,0), CS: decimal(1,0), DC: decimal(1,0), CR: decimal(1,0), FRD: decimal(1,0), TST: decimal(1,0), CC: decimal(1,0), SC: decimal(1,0), PIM: decimal(1,0), LC: decimal(1,0), WEB: decimal(1,0), FL: decimal(1,0), SA: decimal(1,0), URL: decimal(1,0), CV: decimal(1,0), US: decimal(1,0), NOLABEL: decimal(1,0), ... 6 more fields\n",
      "Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 6 more fields]\n",
      "+- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 11 more fields]\n",
      "   +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 10 more fields]\n",
      "      +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 9 more fields]\n",
      "         +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 8 more fields]\n",
      "            +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 7 more fields]\n",
      "               +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 6 more fields]\n",
      "                  +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 5 more fields]\n",
      "                     +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 4 more fields]\n",
      "                        +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 3 more fields]\n",
      "                           +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 2 more fields]\n",
      "                              +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ALL_TEXT#1204]\n",
      "                                 +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_HEADLINE#1004, REVIEW_COMMENTS#1005, REVIEW_RATING#1006, REVIEWER_NICKNAME#1007, REVIEWER_LOCATION#1008, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, ... 6 more fields]\n",
      "                                    +- Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_HEADLINE#1004, REVIEW_COMMENTS#1005, REVIEW_RATING#1006, REVIEWER_NICKNAME#1007, REVIEWER_LOCATION#1008, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, ... 5 more fields]\n",
      "                                       +- Relation[UGC_ID#1002,CREATED_DATE#1003,REVIEW_HEADLINE#1004,REVIEW_COMMENTS#1005,REVIEW_RATING#1006,REVIEWER_NICKNAME#1007,REVIEWER_LOCATION#1008,OBSERVATIONS#1009,PM#1010,NR#1011,PII#1012,PR#1013,CS#1014,DC#1015,CR#1016,FRD#1017,TST#1018,CC#1019,SC#1020,PIM#1021,LC#1022,WEB#1023,FL#1024,SA#1025,... 4 more fields] SnowflakeRelation\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 6 more fields]\n",
      "+- Relation[UGC_ID#1002,CREATED_DATE#1003,REVIEW_HEADLINE#1004,REVIEW_COMMENTS#1005,REVIEW_RATING#1006,REVIEWER_NICKNAME#1007,REVIEWER_LOCATION#1008,OBSERVATIONS#1009,PM#1010,NR#1011,PII#1012,PR#1013,CS#1014,DC#1015,CR#1016,FRD#1017,TST#1018,CC#1019,SC#1020,PIM#1021,LC#1022,WEB#1023,FL#1024,SA#1025,... 4 more fields] SnowflakeRelation\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [UGC_ID#1002, CREATED_DATE#1003, REVIEW_RATING#1006, OBSERVATIONS#1009, PM#1010, NR#1011, PII#1012, PR#1013, CS#1014, DC#1015, CR#1016, FRD#1017, TST#1018, CC#1019, SC#1020, PIM#1021, LC#1022, WEB#1023, FL#1024, SA#1025, URL#1026, CV#1027, US#1028, NOLABEL#1029, ... 6 more fields]\n",
      "+- *(1) Scan SnowflakeRelation [REVIEW_HEADLINE#1004,CV#1027,REVIEW_RATING#1006,PII#1012,FRD#1017,CR#1016,OBSERVATIONS#1009,UGC_ID#1002,REVIEWER_LOCATION#1008,CREATED_DATE#1003,DC#1015,CC#1019,SA#1025,PIM#1021,SC#1020,NR#1011,PM#1010,WEB#1023,PR#1013,LC#1022,CS#1014,URL#1026,NOLABEL#1029,FL#1024,... 4 more fields] PushedFilters: [], ReadSchema: struct<REVIEW_HEADLINE:string,CV:decimal(1,0),REVIEW_RATING:decimal(38,0),PII:decimal(1,0),FRD:de..."
     ]
    }
   ],
   "source": [
    "reviews_4.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facaf1663c0d46f7a7fa2178754f2d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o2158.checkpoint.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 4 times, most recent failure: Lost task 1.3 in stage 21.0 (TID 95, ip-10-101-215-75.us-west-2.compute.internal, executor 2): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1862)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n",
      "\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:624)\n",
      "\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:575)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1862)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 437, in checkpoint\n",
      "    jdf = self._jdf.checkpoint(eager)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o2158.checkpoint.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 4 times, most recent failure: Lost task 1.3 in stage 21.0 (TID 95, ip-10-101-215-75.us-west-2.compute.internal, executor 2): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1862)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n",
      "\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:624)\n",
      "\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:575)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1862)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sc = spark.sparkContext\n",
    "#sc.setCheckpointDir('checkpoint')\n",
    "#reviews_4.checkpoint()\n",
    "reviews_4.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf045b806c1c4ec197d9ca0717a6b78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_5 = spark.createDataFrame(reviews_4.rdd, schema=reviews_4.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdb31f32aae45b88d48d7d792436fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [UGC_ID#4743, CREATED_DATE#4744, REVIEW_RATING#4745, OBSERVATIONS#4746, PM#4747, NR#4748, PII#4749, PR#4750, CS#4751, DC#4752, CR#4753, FRD#4754, TST#4755, CC#4756, SC#4757, PIM#4758, LC#4759, WEB#4760, FL#4761, SA#4762, URL#4763, CV#4764, US#4765, NOLABEL#4766, ... 6 more fields], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "UGC_ID: decimal(38,0), CREATED_DATE: timestamp, REVIEW_RATING: decimal(38,0), OBSERVATIONS: string, PM: decimal(1,0), NR: decimal(1,0), PII: decimal(1,0), PR: decimal(1,0), CS: decimal(1,0), DC: decimal(1,0), CR: decimal(1,0), FRD: decimal(1,0), TST: decimal(1,0), CC: decimal(1,0), SC: decimal(1,0), PIM: decimal(1,0), LC: decimal(1,0), WEB: decimal(1,0), FL: decimal(1,0), SA: decimal(1,0), URL: decimal(1,0), CV: decimal(1,0), US: decimal(1,0), NOLABEL: decimal(1,0), ... 6 more fields\n",
      "LogicalRDD [UGC_ID#4743, CREATED_DATE#4744, REVIEW_RATING#4745, OBSERVATIONS#4746, PM#4747, NR#4748, PII#4749, PR#4750, CS#4751, DC#4752, CR#4753, FRD#4754, TST#4755, CC#4756, SC#4757, PIM#4758, LC#4759, WEB#4760, FL#4761, SA#4762, URL#4763, CV#4764, US#4765, NOLABEL#4766, ... 6 more fields], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [UGC_ID#4743, CREATED_DATE#4744, REVIEW_RATING#4745, OBSERVATIONS#4746, PM#4747, NR#4748, PII#4749, PR#4750, CS#4751, DC#4752, CR#4753, FRD#4754, TST#4755, CC#4756, SC#4757, PIM#4758, LC#4759, WEB#4760, FL#4761, SA#4762, URL#4763, CV#4764, US#4765, NOLABEL#4766, ... 6 more fields], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan ExistingRDD[UGC_ID#4743,CREATED_DATE#4744,REVIEW_RATING#4745,OBSERVATIONS#4746,PM#4747,NR#4748,PII#4749,PR#4750,CS#4751,DC#4752,CR#4753,FRD#4754,TST#4755,CC#4756,SC#4757,PIM#4758,LC#4759,WEB#4760,FL#4761,SA#4762,URL#4763,CV#4764,US#4765,NOLABEL#4766,... 6 more fields]"
     ]
    }
   ],
   "source": [
    "reviews_5.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e250ea334d14ecc86911d30c30ed94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[UGC_ID: decimal(38,0), CREATED_DATE: timestamp, REVIEW_RATING: decimal(38,0), OBSERVATIONS: string, PM: decimal(1,0), NR: decimal(1,0), PII: decimal(1,0), PR: decimal(1,0), CS: decimal(1,0), DC: decimal(1,0), CR: decimal(1,0), FRD: decimal(1,0), TST: decimal(1,0), CC: decimal(1,0), SC: decimal(1,0), PIM: decimal(1,0), LC: decimal(1,0), WEB: decimal(1,0), FL: decimal(1,0), SA: decimal(1,0), URL: decimal(1,0), CV: decimal(1,0), US: decimal(1,0), NOLABEL: decimal(1,0), ALL_TEXT: string, PRICE: int, finished_TOKEN: array<string>, finished_TOKEN_NOSTOPWORDS: array<string>, finished_NORMALIZED: array<string>, finished_LEMMA: array<string>]"
     ]
    }
   ],
   "source": [
    "reviews_5.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256f261b09564b748763320586e8fffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o2310.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 22.0 failed 4 times, most recent failure: Lost task 1.3 in stage 22.0 (TID 108, ip-10-101-215-75.us-west-2.compute.internal, executor 9): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\n",
      "\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\n",
      "\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:92)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o2310.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 22.0 failed 4 times, most recent failure: Lost task 1.3 in stage 22.0 (TID 108, ip-10-101-215-75.us-west-2.compute.internal, executor 9): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\n",
      "\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\n",
      "\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:92)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.NullPointerException\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WORKS FINE ON SMALL DATA SETS BUT ERRORS ON LARGE ONES\n",
    "#m5a.2xlarge master + m5a.2xlarge node: 30k rows works with cache, 60k with cache DOESN'T, \n",
    "#60k with change to rdd - works (10 features = 130 secs, 1000 features = 128 secs) \n",
    "#100k rows - rdd - 1000 features - error\n",
    "#m5a.2xlarge master + m5a.2xlarge FIVE nodes: 100k - rdd - error, 80k - rdd - error\n",
    "#m5.12xlarge master + m5.12xlarge node: 200k - rdd - error, 100k - rdd - error, 100k - cache - fail, 100k - rdd and cache - fail, 100k - checkpoint \n",
    "\n",
    "#An error occurred while calling o2069.fit.\n",
    "#: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 4 times, most recent failure: Lost task 0.3 in stage 33.0 (TID 59, ip-10-101-214-119.us-west-2.compute.internal, executor 6): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
    "\n",
    "#TF-IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol='finished_LEMMA', outputCol='RAW_TFIDF_FEATURES', numFeatures=1000)\n",
    "featurized_data = hashingTF.transform(reviews_4)\n",
    "#featurized_data.show()\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol='RAW_TFIDF_FEATURES', outputCol='TFIDF_FEATURES')\n",
    "idf_model = idf.fit(featurized_data)\n",
    "#idf_model.show()\n",
    "tfidf_results = idf_model.transform(featurized_data)\n",
    "\n",
    "tfidf_results.select('finished_LEMMA','TFIDF_FEATURES').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575a8390267143e38113d945972f46e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+-------------+-----+----------------------------------------------------------------------------------------------------+\n",
      "|ALL_TEXT                                                                                                                          |TFIDF_FEATURES                                                                                                 |REVIEW_RATING|PRICE|features                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+-------------+-----+----------------------------------------------------------------------------------------------------+\n",
      "|headline Really good comment It's a really good product that is comfortable and well built nickname Zaakir004 location undisclosed|(10,[0,1,2,4,6,7,8],[0.0,0.0,0.22723387798204245,0.0,0.3889412688318618,0.8622323877651492,0.3677111103211877])|5            |0    |(12,[2,6,7,8,10],[0.22723387798204245,0.3889412688318618,0.8622323877651492,0.3677111103211877,5.0])|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+-------------+-----+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "#Append additional numeric columns to existing TF-IDF features vector\n",
    "vectorAssembler = VectorAssembler(inputCols = ['TFIDF_FEATURES','REVIEW_RATING','PRICE'], outputCol = 'features')\n",
    "test = vectorAssembler.transform(tfidf_results)\n",
    "test.select('ALL_TEXT','TFIDF_FEATURES','REVIEW_RATING','PRICE','features').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f96711384b84da48a7ec5d787129e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "|  0.0|(20,[4,8,13,15,18...|\n",
      "+-----+--------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\"),\n",
    "    (0.0, \"Logistic regression models are dumb\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b0054637574c14bcbd56074d269b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|label|sentence                           |words                                     |rawFeatures                              |features                                                                                                              |\n",
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |(20,[0,5,9,17],[1.0,1.0,1.0,2.0])        |(20,[0,5,9,17],[0.9162907318741551,0.9162907318741551,0.5108256237659907,1.8325814637483102])                         |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|(20,[2,7,9,13,15],[1.0,1.0,3.0,1.0,1.0]) |(20,[2,7,9,13,15],[0.9162907318741551,0.9162907318741551,1.5324768712979722,0.22314355131420976,0.22314355131420976]) |\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |(20,[4,6,13,15,18],[1.0,1.0,1.0,1.0,1.0])|(20,[4,6,13,15,18],[0.5108256237659907,0.9162907318741551,0.22314355131420976,0.22314355131420976,0.5108256237659907])|\n",
      "|0.0  |Logistic regression models are dumb|[logistic, regression, models, are, dumb] |(20,[4,8,13,15,18],[1.0,1.0,1.0,1.0,1.0])|(20,[4,8,13,15,18],[0.5108256237659907,0.9162907318741551,0.22314355131420976,0.22314355131420976,0.5108256237659907])|\n",
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "rescaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610d1b7d056e4a84b78a067bdceb93ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (20,[6],[0.6599674909832574])\n",
      "Intercept: -1.1875233653399353"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = rescaledData\n",
    "\n",
    "lr = LogisticRegression(maxIter=3, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddee7c80158c48bd94259d18883e68ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[0,5,9,17],[1...|(20,[0,5,9,17],[0...|[1.18752336533993...|[0.76629782750481...|       0.0|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[2,7,9,13,15]...|(20,[2,7,9,13,15]...|[1.18752336533993...|[0.76629782750481...|       0.0|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[4,6,13,15,18...|(20,[4,6,13,15,18...|[0.58280127001373...|[0.64171172349340...|       0.0|\n",
      "|  0.0|Logistic regressi...|[logistic, regres...|(20,[4,8,13,15,18...|(20,[4,8,13,15,18...|[1.18752336533993...|[0.76629782750481...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+"
     ]
    }
   ],
   "source": [
    "#Show what labels the model predicted\n",
    "lrModel_2 = lrModel.transform(training)\n",
    "lrModel_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5bb1ae4f1e46a6bfd497b8ad58d658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.5623351446188085\n",
      "0.5501013689318484\n",
      "0.5421128362877067\n",
      "0.5315519433187645"
     ]
    }
   ],
   "source": [
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8651793e224b4b9482ae23957a54bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      "True positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0"
     ]
    }
   ],
   "source": [
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a12d0f3f2644b00b4bbc8ef440550da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision by label:\n",
      "label 0: 0.75\n",
      "label 1: 0.0\n",
      "Recall by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      "F-measure by label:\n",
      "label 0: 0.8571428571428571\n",
      "label 1: 0.0"
     ]
    }
   ],
   "source": [
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dab4776e0746cfb56046539f021c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "FPR: 0.75\n",
      "TPR: 0.75\n",
      "F-measure: 0.6428571428571428\n",
      "Precision: 0.5625\n",
      "Recall: 0.75"
     ]
    }
   ],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split - use this over cross fold since it will run quicker even though potentially less accurate results\n",
    "#Large enough data set should account for not using cross fold\n",
    "\n",
    "train, test = reviews.randomSplit([0.9, 0.1], seed=12345)\n",
    "train.show()\n",
    "train.count()\n",
    "#884/1000 = 88%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
